import { SAMPLE_RATE } from '../constants';

/**
 * Decodes a base64 string into a Uint8Array.
 */
export const decodeBase64 = (base64: string): Uint8Array => {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
};

/**
 * Converts raw PCM data (16-bit, mono) into a WAV file Blob.
 * Gemini TTS typically returns raw PCM without headers.
 */
export const pcmToWav = (pcmData: Uint8Array, sampleRate: number = SAMPLE_RATE): Blob => {
  const numChannels = 1;
  const byteRate = sampleRate * numChannels * 2; // 2 bytes per sample (16-bit)
  const blockAlign = numChannels * 2;
  const dataSize = pcmData.length;
  const headerSize = 44;
  const totalSize = headerSize + dataSize;

  const buffer = new ArrayBuffer(totalSize);
  const view = new DataView(buffer);

  // RIFF chunk
  writeString(view, 0, 'RIFF');
  view.setUint32(4, 36 + dataSize, true); // File size - 8
  writeString(view, 8, 'WAVE');

  // fmt chunk
  writeString(view, 12, 'fmt ');
  view.setUint32(16, 16, true); // Subchunk1Size (16 for PCM)
  view.setUint16(20, 1, true); // AudioFormat (1 for PCM)
  view.setUint16(22, numChannels, true); // NumChannels
  view.setUint32(24, sampleRate, true); // SampleRate
  view.setUint32(28, byteRate, true); // ByteRate
  view.setUint16(32, blockAlign, true); // BlockAlign
  view.setUint16(34, 16, true); // BitsPerSample

  // data chunk
  writeString(view, 36, 'data');
  view.setUint32(40, dataSize, true); // Subchunk2Size

  // Write PCM data
  const resultBytes = new Uint8Array(buffer);
  resultBytes.set(pcmData, 44);

  return new Blob([buffer], { type: 'audio/wav' });
};

/**
 * Merges multiple WAV blobs into a single WAV blob.
 * Assumes all blobs have the same format (created by pcmToWav with 44 byte header).
 */
export const mergeWavBlobs = async (blobs: Blob[]): Promise<Blob> => {
  if (blobs.length === 0) throw new Error("No audio blobs to merge");
  
  // 1. Get ArrayBuffers from Blobs
  const buffers = await Promise.all(blobs.map(b => b.arrayBuffer()));
  
  // 2. Extract PCM data (skip 44-byte WAV header)
  // We assume these were generated by our app, so header is exactly 44 bytes.
  const pcms = buffers.map(b => new Uint8Array(b.slice(44)));
  
  // 3. Calculate total length
  const totalLength = pcms.reduce((acc, curr) => acc + curr.length, 0);
  
  // 4. Concatenate
  const combinedPcm = new Uint8Array(totalLength);
  let offset = 0;
  for (const pcm of pcms) {
    combinedPcm.set(pcm, offset);
    offset += pcm.length;
  }
  
  // 5. Wrap in new WAV header
  return pcmToWav(combinedPcm);
};

const writeString = (view: DataView, offset: number, string: string) => {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
};

// --- New Utilities for Client-Side Splitting ---

/**
 * Splits a large audio blob into smaller WAV chunks.
 * Resamples to 16kHz Mono to maximize efficiency (approx 2MB per minute).
 */
export const splitAudioBlob = async (blob: Blob, chunkDurationSec: number = 300): Promise<Blob[]> => {
  // 1. Decode Audio
  const arrayBuffer = await blob.arrayBuffer();
  // Using a fallback for Safari/Old browsers webkitAudioContext
  const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
  const ctx = new AudioContextClass();
  
  // Decode can be memory intensive for huge files, but 45MB mp3 is manageable (~300-400MB raw PCM)
  const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
  
  const chunks: Blob[] = [];
  const totalDuration = audioBuffer.duration;
  const sampleRate = 16000; // Force 16kHz to keep file size small for API
  
  let currentTime = 0;
  
  while (currentTime < totalDuration) {
    const duration = Math.min(chunkDurationSec, totalDuration - currentTime);
    const chunkBlob = await encodeAudioBufferSliceToWav(audioBuffer, currentTime, duration, sampleRate);
    chunks.push(chunkBlob);
    currentTime += duration;
  }
  
  // Close context to free resources
  if(ctx.state !== 'closed') ctx.close();
  
  return chunks;
};

/**
 * Helper to slice an AudioBuffer, downsample it, and convert to WAV Blob.
 */
async function encodeAudioBufferSliceToWav(
  sourceBuffer: AudioBuffer, 
  startSec: number, 
  durationSec: number, 
  targetSampleRate: number
): Promise<Blob> {
  const sourceRate = sourceBuffer.sampleRate;
  const startFrame = Math.floor(startSec * sourceRate);
  const endFrame = Math.floor((startSec + durationSec) * sourceRate);
  const frameCount = endFrame - startFrame;
  const sourceData = sourceBuffer.getChannelData(0); // Mono extraction (left channel)
  
  // Downsampling logic (simple linear interpolation or skipping)
  // To keep it fast and simple, we'll use an OfflineAudioContext to do the resampling for us
  // This is much better quality than manual array skipping.
  
  const offlineCtx = new OfflineAudioContext(1, durationSec * targetSampleRate, targetSampleRate);
  const source = offlineCtx.createBufferSource();
  
  // Create a new buffer for just this slice at original rate
  const sliceBuffer = offlineCtx.createBuffer(1, frameCount, sourceRate);
  sliceBuffer.copyToChannel(sourceData.subarray(startFrame, endFrame), 0);
  
  source.buffer = sliceBuffer;
  source.connect(offlineCtx.destination);
  source.start();
  
  const renderedBuffer = await offlineCtx.startRendering();
  
  // Convert Float32 to Int16 PCM
  const floatData = renderedBuffer.getChannelData(0);
  const int16Data = new Int16Array(floatData.length);
  
  for (let i = 0; i < floatData.length; i++) {
    // Clamp and scale
    const s = Math.max(-1, Math.min(1, floatData[i]));
    int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
  }
  
  // Convert Int16Array to Uint8Array for pcmToWav
  const uint8Bytes = new Uint8Array(int16Data.buffer);
  
  // Use existing pcmToWav helper
  return pcmToWav(uint8Bytes, targetSampleRate);
}